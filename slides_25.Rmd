---
title: "Statistiques computationnelles"
author: <font size="5"> Charlotte Baey </font>
date: <font size="5"> M1 MA - 2024/2025 </font>
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, hygge]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%"
---

<style>

.remark-slide-content {
  background-color: #FFFFFF;
  border-top: 80px solid #16A085;
  font-size: 20px;
  line-height: 1.5;
  padding: 1em 2em 1em 2em
}

.my-one-page-font {
  font-size: 20px;
}

.remark-slide-content > h1 {
  font-size: 38px;
  margin-top: -85px;
}

.inverse {
  background-color: #16A085;
  border-top: 80px solid #16A085;
  text-shadow: none;
	background-position: 50% 75%;
  background-size: 150px;
  font-size: 40px
}

.title-slide {
  background-color: #16A085;
  border-top: 80px solid #16A085;
  background-image: none;
}

.remark-slide-number {
  position: absolute;
}

.remark-slide-number .progress-bar-container {
  position: absolute;
  bottom: 0;
  height: 4px;
  display: block;
  left: 0;
  right: 0;
}

.remark-slide-number .progress-bar {
  height: 100%;
  background-color: grey;
}

.left-column {
  width: 20%;
  height: 92%;
  float: left;
}
.left-column h2:last-of-type, .left-column h3:last-child {
  color: #000;
}
.right-column {
  width: 75%;
  float: right;
  padding-top: 1em;
}


.left-column2 {
  width: 60%;
  height: 92%;
  float: left;
}
.right-column2 {
  width: 35%;
  height: 92%;
  float: left;
}

</style>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      yellow: ["{\\color{yellow}{#1}}", 1],
      orange: ["{\\color{orange}{#1}}", 1],
      green: ["{\\color{green}{#1}}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
</script>

```{css, echo=FALSE}
.left-code {
  width: 40%;
  height: 92%;
  float: left;
}
.right-plot {
  width: 59%;
  float: right;
  padding-left: 1%;
}
```

```{css, echo=FALSE}
.left-plot {
  width: 59%;
  float: left;
}
.right-code {
  width: 40%;
  float: right;
  padding-left: 1%;
}
```


```{r setup, include = FALSE}

# R markdown options
#knitr::opts_chunk$set(echo = TRUE, 
                      #cache = TRUE, 
                      #fig.width = 10,
                      #fig.height = 5,
                      #fig.align = "center", 
                      #message = FALSE)

# Load packages
library(gapminder)
library(gganimate)
library(ggplot2)
library(tidyverse)
library(xaringancolor)
library(flair)
yellow <- "#FFCC29"
orange <- "#F58634"
green <- "#007965"
```

```{r, echo=FALSE}
library(reticulate)
Sys.setenv('RETICULATE_PYTHON'='/usr/bin/python3')
```

# Quelques informations pratiques

### Plan du cours
1. Méthodes de ré-échantillonnage
2. Méthodes de Monte-Carlo
3. Introduction aux statistiques bayésiennes
4. Algorithme EM (s'il reste du temps)

### Organisation

- 2 séances de cours d'1h30 par semaine ($\times$ 11 semaines)
- 2 séances de TD/TP de 2h par semaine ($\times$ 12 semaines)

### Evaluation

- 1 DS intermédiaire d'une durée de 2h
- 1 Projet **à effectuer en binôme**
- 1 DS final d'une durée de 3h

.red[**Aucun document autorisé lors des examens.**]

---
# Sommaire

</br>
**1. Méthodes de ré-échantillonnage**
  - [Cours 1](#c1) (13/01/2025)
  - [Cours 2](#c2) (14/01/2025)
  - [Cours 3](#c3) (20/01/2025)
  - [Cours 4](#c4) (21/01/2025)
  - [Cours 5](#c5) (27/01/2025)
  
**2. Méthodes de Monte-Carlo**
  - [Cours 6](#c6) (28/01/2025)
  - [Cours 7](#c7) (03/02/2025)
  - [Cours 7](#c8) (04/02/2025)

---
name: c1
class: inverse, middle, center

# Introduction


---
class: my-one-page-font 
# C'est quoi les statistiques computationnelles ?

<br>

<br>

 - C'est le recours (plus ou moins) intensif à l'ordinateur pour répondre à des questions statistiques que l'on ne sait pas (ou difficilement) résoudre autrement.
 
 - On utilise/développe/étudie des algorithmes, des astuces numériques/statistiques/computationnelles 
 - L'objectif est de faire de l'inférence, d'étudier la robustesse de méthodes statistiques, de traiter de grands jeux de données, ...
 
 

---

class: inverse, middle, center

# I. Méthodes de ré-échantillonnage

---

# Notion d'échantillon

Qu'est-ce qu'un échantillon ?

- une suite de variables aléatoires $\mathcal{X} = (X_1, \dots, X_n)$
- dont on observe une réalisation $\mathcal{X}(\omega) = (X_1(\omega), \dots, X_n(\omega))$

---
# Notion d'échantillon

Qu'est-ce qu'un échantillon ?

- une suite de variables aléatoires $\mathcal{X} = (X_1, \dots, X_n)$
- dont on observe une réalisation $\mathcal{X}(\omega) = (X_1(\omega), \dots, X_n(\omega))$

Que représente $\omega$ ?
- l'aléa autour de l'expérience (ex. : $n$ lancers d'une pièce de monnaie)
- cet aléa $\omega \in \Omega$ est transporté dans $\mathbb{R}$ via $X_i$
<!-- - on a seulement accès à la variabilité sur $\mathbb{R}$ -->
- en général, on ne dispose que d'une seule réalisation, pour un $\omega$ donné

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=34)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=35)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=36)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=37)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=38)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=39)
```


---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=40)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=41)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=42)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=43)
```


---
# Un exemple simple 

Soit $(X_1,\dots,X_n)$ un échantillon gaussien i.i.d. de loi $\mathcal{N}(\theta,1)$, et $\hat{\theta}$ l'EMV de $\theta$. Quelle est la loi de $\hat{\theta}$ ?

--

.pull-left[
```{r, eval=F}
theta_vrai <- 2; n <- 100; N <- 500
hat_theta <- rep(0,N)
for (i in 1:N){
  ech_i <- rnorm(n,theta_vrai,1)
  hat_theta[i] <- mean(ech_i)
}
hist(hat_theta,freq=F)
curve(dnorm(x,theta_vrai,1/sqrt(n)))
```
]
.pull-right[
```{r, fig.height=5,fig.width=6,fig.retina=2,results='hold',echo=F}
theta_vrai <- 2; n <- 100; N <- 500
hat_theta <- rep(0,N)
for (i in 1:N){
  ech_i <- rnorm(n,theta_vrai,1)
  hat_theta[i] <- mean(ech_i)
}
hist(hat_theta,freq=F,main="Histogramme des estimations",ylim=c(0,4))
curve(dnorm(x,theta_vrai,1/sqrt(n)),
      lwd=2,col="red",add=T)
```
]
---
class: my-one-page-font 
# Jackknife

Comment construire de nouveaux échantillons ?

```{r, fig.height=6,fig.width=10,fig.retina=2, echo=F}
library(RColorBrewer)
pal <- brewer.pal(n = 8, name = "Dark2")
n <- 8
set.seed(2)
ech <- sort(rnorm(n,0,1))
ech_j <- matrix(0,nrow=n,ncol=7)
for (i in 1:n){
  ech_j[i,] <- ech[-i]
}

mean_ech <- mean(ech)
mean_j <- apply(ech_j,1,mean)
par(oma=c(0,4,0,0))
plot(ech,rep(1.9,n),col=pal,pch=19,cex=4,ylim=c(0.1,2),yaxt="none",xlab="Observations",ylab="")
points(mean_ech,1.9,col="red",pch=15,cex=2)
mtext("éch. initial",side=2,at=1.9,las=1,adj=1.1)
```

---
class: my-one-page-font 
# Jackknife

Comment construire de nouveaux échantillons ?

```{r, fig.height=6,fig.width=10,fig.retina=2, echo=F}
par(oma=c(0,4,0,0))
plot(ech,rep(1.9,n),col=pal,pch=19,cex=4,ylim=c(0.1,2),yaxt="none",xlab="Observations",ylab="")
points(mean_ech,1.9,col="red",pch=15,cex=2)
mtext("éch. initial",side=2,at=1.9,las=1,adj=1.1)
for (i in 1:n){
  abline(h=1.8-0.2*i,col="grey")
  points(ech_j[i,],rep(1.8-0.2*i,n-1),col=pal[-i],cex=3,pch=19)
  points(mean_j[i],1.8-0.2*i,col="red",pch=15,cex=2)
  mtext(paste("éch. jackknife",i),side=2,at=1.8-0.2*i,las=1,adj=1.1)
}
```


---
# Jackknife 

#### 1. Réduction du biais

 - Estimation du biais : 

$$\hat{b}_{jack} = (n-1)(\hat{\theta}_{jack} - \hat{\theta}),$$ avec $\hat{\theta}_{jack} = \frac{1}{n}\sum_{i=1}^n \hat{\theta}_{(i)}$.

 - Pseudo-valeurs :

$$\tilde{\theta}_{(i)} = n\hat{\theta} - (n-1)\hat{\theta}_{(i)}$$
<!-- Sur l'exemple précédent, cela donne : -->
 <!-- \tilde{\theta}_{(1)} = \tilde{\theta}_{(2)} = \tilde{\theta}_{(3)} = \tilde{\theta}_{(7)} = \tilde{\theta}_{(8)} = \tilde{\theta}_{(10)} = \tilde{\theta}_{(12)} = 13\times0.5384 - 12\times0.50 = 0.9992
 \tilde{\theta}_{(4)} = \tilde{\theta}_{(5)} = \tilde{\theta}_{(6)} = \tilde{\theta}_{(9)} = \tilde{\theta}_{(11)} = \tilde{\theta}_{(13)} = 13\times0.5384 - 12\times0.5833 = -0.0004 -->


 - Estimateur jackknife corrigé du biais :

$$\hat{\theta}^*_{jack} = \hat{\theta} - \hat{b}_{jack} = \frac{1}{n}\sum_{i=1}^n \tilde{\theta}_{(i)}$$
.red[**Réduire le biais n'implique pas nécessairement une amélioration de l'estimateur (au sens du risque quadratique)**]


---
# Jackknife

#### 2. Estimation de la variance

 $$\hat{s}^2_{jack} = \frac{n-1}{n} \sum_{i=1}^n \big(\hat{\theta}_{(i)} - \hat{\theta}^*_{jack}\big)^2$$
 
avec les pseudo-valeurs : $\hat{s}^2_{jack} = \frac{1}{n(n-1)} \sum_{i=1}^n \big(\tilde{\theta}_{(i)} - \tilde{\theta}\big)^2$

--

#### 3. Construction d'intervalles de confiance

Si existence d'un TCL :
- en utilisant l'estimateur jackknife de la variance : 
 $$\hat{I}_{jack} = \left[\hat{\theta} - q^{\mathcal{N(0,1)}}_{1-\alpha/2} \hat{s}_{jack} ; \hat{\theta} + q^{\mathcal{N(0,1)}}_{1-\alpha/2} \hat{s}_{jack} \right]$$

- en utilisant les deux estimateurs :
  $$\hat{I}_{jack} = \left[\hat{\theta}_{jack} - q^{\mathcal{N(0,1)}}_{1-\alpha/2} \hat{s}_{jack} ; \hat{\theta}_{jack} + q^{\mathcal{N(0,1)}}_{1-\alpha/2} \hat{s}_{jack} \right]$$

<span style="color:#16A085">**fin du cours 1 (13/01/2025)**</span>


---
name: c2
# Résumé et remarques

<br>
- le jackknife est une méthode **non-paramétrique** permettant d'estimer le biais et la variance d'un estimateur à l'aide de simulations 

- la consistance est garantie pour un grand nombre d'estimateurs **suffisamment réguliers**

- les hypothèses de régularité sont toutefois plus strictes que pour un TCL par exemple :
 - la delta-méthode requiert la différentiabilité de $g$ en $\mu=\mathbb{E}(X_1)$
 - la consistance de $\hat{s}_{jack}$ requiert que $g'$ soit continue en $\mu$
 - ex. d'estimateur pour lequel $\hat{s}_{jack}$ n'est pas consistant : la médiane empirique (malgré existence TCL)
 
- extensions possibles reposant sur des hypothèses moins fortes : 
 - le *delete-d* jackknife
 - le jackknife infinitésimal
 <!-- - peut-être utilisé pour la médiane empirique -->
 <!-- - peut permettre d'estimer la distribution de $\hat{\theta}$ -->

<!--</br>
<span style="color:#16A085">**fin du cours 1 (22/01/2024)**</span>-->

---
# Du jackknife au bootstrap

 - $\hat{\theta} = T(\underbrace{X_1,\dots,X_n}_{\text{observations}},\underbrace{\omega_1,\dots,\omega_n}_{\text{poids des obs.}}) = T(X_1,\dots,X_n,\frac{1}{n},\dots,\frac{1}{n})$
 
 - réplication jackknife : $\hat{\theta}_{(i)} = T(X_1,\dots,X_{i-1},X_i,X_{i+1},X_n,\frac{1}{n-1},\dots,\frac{1}{n-1},0,\frac{1}{n-1},\dots,\frac{1}{n-1})$

--

 - Idée du bootstrap : mettre des poids **aléatoires**

--

 - Procédure de ré-échantillonnage : tirer uniformément et **avec remise** parmi les observations de $\mathcal{X}$, pour construire un échantillon *bootstrap* de taille $n$ noté $\mathcal{X}^*$ :
 
 - Puis sur chaque échantillon bootstrap, on construit la statistique bootstrapée $\hat{\theta}^* = T(\mathcal{X}^*)$

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=25)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=26)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=27)
```


---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=28)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=29)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=30)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=31)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/Figures/boot_samples.pdf")
```



---
# Fonction de répartition empirique

 
```{r, echo=FALSE}
set.seed(10101)
xi <- rnorm(10)
print(round(xi,2))
```

--

```{r, echo=FALSE, warning=FALSE, fig.height=7, fig.width=10,fig.retina=2}
plot(ecdf(xi),xaxt='n',main="",col="white")
mtext(round(xi,2),at=xi,side=1,line=1,cex=0.8)
mtext(paste0("X(",1:10,")"),at=xi,side=1,line=2,cex=0.75)
axis(1,at=sort(xi),labels=rep("",10),cex.axis=0.75)
```

---
# Fonction de répartition empirique

 
```{r, echo=FALSE}
set.seed(10101)
xi <- rnorm(10)
print(round(xi,2))
```


```{r, echo=FALSE, warning=FALSE, fig.height=7, fig.width=10,fig.retina=2}
plot(ecdf(xi),xaxt='n',main="")
mtext(round(xi,2),at=xi,side=1,line=1,cex=0.8)
mtext(paste0("X(",1:10,")"),at=xi,side=1,line=2,cex=0.75)
axis(1,at=sort(xi),labels=rep("",10),cex.axis=0.75)
```

---

# Fonction de répartition empirique

 - Echantillon exponentiel
 
```{r, echo=FALSE, warning=FALSE, fig.height=4, fig.width=16,fig.retina=2}
n <- c(10,20,50,100,200,500)
d <- lapply(n,FUN = function(i){return(data.frame(x=rexp(i*100,1),rep=rep(1:100,each=i),n=rep(i,i*100)))})
d <- do.call(rbind,d)

d %>% ggplot(aes(x,group=rep)) + stat_ecdf(geom="step",col="darkgrey") + stat_function(fun=pexp) + facet_wrap(~n,nrow = 1) 
```

 - Echantillon uniforme
 
```{r, echo=FALSE, warning=FALSE, fig.width=16, fig.height=4,fig.retina=2}
d <- lapply(n,FUN = function(i){return(data.frame(x=runif(i*100),rep=rep(1:100,each=i),n=rep(i,i*100)))})
d <- do.call(rbind,d)

d %>% filter(n<1000) %>% ggplot(aes(x,group=rep)) + stat_ecdf(geom="step",col="darkgrey") + stat_function(fun=punif) + facet_wrap(~n,nrow=1) + xlim(c(-0.2,1.2))
```

---
# Bootstrap

.pull-left[
### Monde réel
.left[
<span style="color:white">**on raisonne conditionnellement à $F_n$**</span>
- échantillon $\mathcal{X} = (X_1,\dots,X_n)$
- $X_i$ de loi inconnue $F$
- paramètre $\theta(F)$
- estimateur $\hat{\theta} = T(\mathcal{X})$
- loi de $\hat{\theta}$ : $G$ inconnue
]
]
.pull-right[
### Monde Bootstrap 
.left[
<span style="color:red">**on raisonne conditionnellement à $F_n$**</span>
- échantillon bootstrap $\mathcal{X}^* = (X_{1}^*,\dots,X_{n}^*)$
- $X_{i}^*$ de loi connue $F_n$
- paramètre $\theta(F_n) = \hat{\theta}$
- statistique bootstrapée $\hat{\theta}^* = T(\mathcal{X}^*)$
- loi de $\hat{\theta}^*$ : $G^*$ connue
]
]

</br>
.center[
$G$ inconnue $\longrightarrow$ $G^*$ connue $\longrightarrow$ $\hat{G}^*_B$ approximation bootstrap
]

</br>
<span style="color:#16A085">**fin du cours 2 (14/01/2025)**</span>


---
name: c3
# Bootstrap : principe du plug-in

 - on remplace $F$ par sa version empirique $F_n$ 


```{r, echo=FALSE, fig.width=12, fig.height=6,fig.retina=2}
set.seed(1999)
x <- rnorm(15)
par(mfrow=c(1,2))
curve(pnorm(x),from=-2.5,to=2.5,xlab="",ylab="",main=expression(Loi~des~X[i]))
plot(ecdf(x),xaxt='n',xlim=c(-2.5,2.5),main=expression(Loi~des~X[bi]^"*"))
text(sort(x),y = par("usr")[3]-0.075,labels=paste0("X(",1:15,")"),xpd=NA,cex=0.75,srt=45)
axis(1,at=sort(x),labels=rep("",15),cex.axis=0.75)
lines(seq(-3,3,0.1),pnorm(seq(-3,3,0.1)),xlab="",ylab="",col="grey")
```
<!-- .pull-left[ -->
<!-- .center[ -->
<!-- espérance : $\theta = \int x dF(x)$ ]]-->
<!-- .pull-right[ -->
<!-- espérance : $\int x dF_n(x) = \frac{1}{n} \sum_{i=1}^n X_i$] -->

---
# Bootstrap : principe du plug-in

#### Exemple  
$\theta = \mathbb{E}(X_1) = \int x dF(x)$ 

--

Méthode du plug-in : on estime $\theta$ par $\hat{\theta} = \int x dF_n(x) = \frac{1}{n}\sum_{i=1}^n X_i$.

--

Dans l'échantillon bootstrap, les $X_{b,i}^*$ sont distribuées selon la loi $F_n$, et ont pour espérance

$\theta^* = \int x dF_n(x) = \hat{\theta}$

--

La statistique bootstrapée $\hat{\theta}^*_b = \frac{1}{n}\sum_{i=1}^n X_{b,i}^*$ est donc à $\theta^*$ ce que $\hat{\theta}$ est à $\theta$.

---
# Bootstrap 

#### Exemple 2 : estimation du biais de $\hat{\theta}$

Paramètre $\theta(F)$ estimé par $\hat{\theta} = \theta(F_n)$

--

Biais : $b(\hat{\theta}) = \mathbb{E}_F(\hat{\theta}) - \theta = \int xdG(x) - \theta(F)$

--

Dans le monde bootstrap : 

$$b^*(\hat{\theta}) = \int x dG^*(x) - \theta(F_n) = \int x dG^*(x) - \hat{\theta}$$

--

Estimateur du bootstrap :

$$\hat{b}^*(\hat{\theta}) = \int x d\hat{G}^*_B(x) - \hat{\theta} = \frac{1}{B} \sum_{i=1}^B \hat{\theta}^*_b - \hat{\theta}$$

---
# Bootstrap

#### Exemple 3 : estimation de la variance de $\hat{\theta}$

Variance : $\text{Var} (\hat{\theta}) = \mathbb{E}_F\big[(\hat{\theta} - \mathbb{E}(\hat{\theta}))^2\big] = \int (x - \int x dG(x))^2dG(x)$

--

Dans le monde bootstrap :

$$\text{Var}^*(\hat{\theta}) = \int (x - \int x dG^*(x))^2 dG^*(x)$$

--

Estimateur du bootstrap :

$$\hat{\text{Var}}^*(\hat{\theta}) = \int (x - \int x d\hat{G}_B^*(x))^2 d\hat{G}_B^*(x) = \frac{1}{B} \sum_{b=1}^B \big(\hat{\theta}^*_b - \frac{1}{B} \sum_{b=1}^B \hat{\theta}^*_b \big)^2$$

$\rightarrow$ c'est la variance empirique des statistiques bootstrapées $\hat{\theta}^*_1,\dots,\hat{\theta}^*_B$.

</br>
<span style="color:#16A085">**fin du cours 3 (20/01/2025)**</span>

---
name: c4
# Construction d'intervalles de confiance

#### Méthode du bootstrap classique
Point de départ : l'existence d'une quantité pivotale, par exemple $\hat{\theta} - \theta$, de loi $H$.

--
 
```{r echo=FALSE, fig.width=5,fig.height=5,warning=FALSE,fig.retina=2} 
library(ggplot2)
x <- seq(0,13,0.01)
y <- dgamma(x,2.5,1)
d <- data.frame(x=rep(x,2),y=rep(y,2),type=rep(c("HDI","Quantiles"),each=length(x)),
                xmin=c(qgamma(0.025,2.5,1)),xmax=c(qgamma(0.975,2.5,1)))
ggplot(data=d,aes(x,y)) + geom_line() + stat_function(fun = dgamma,args = list(shape=2.5, rate=1),geom = "area",
                                                         fill = "red", alpha = 0.5, xlim = c(0,qgamma(0.025,2.5,1))) +
                                        stat_function(fun = dgamma,args = list(shape=2.5, rate=1),geom = "area",
                                                         fill = "red", alpha = 0.5, xlim = c(qgamma(0.975,2.5,1),10)) +
  ggtitle(expression(paste("Loi (densité) de ",hat(theta)-theta))) + xlim(c(-0.75,10)) + annotate(geom="text",x=qgamma(0.025,2.5,1)+0.15,y=-0.015,label="H^-1~(alpha/2)",parse=TRUE) + geom_vline(xintercept = 0) + geom_segment(x=qgamma(0.025,2.5,1),xend=qgamma(0.025,2.5,1),y=-0.005,yend=0.005) + geom_hline(yintercept = 0) + 
  annotate(geom="text",x=qgamma(0.975,2.5,1),y=-0.01,label="H^-1~(1-alpha/2)",parse=TRUE) + 
  geom_segment(x=qgamma(0.975,2.5,1),xend=qgamma(0.975,2.5,1),y=-0.005,yend=0.005) +
  geom_segment(x=7,xend=8,y=0.005,yend=0.07,lwd=0.1) + geom_segment(x=0.25,xend=-0.5,y=0.0075,yend=0.07,lwd=0.1) +
  annotate(geom="text",x=-0.55,y=0.08,label="alpha/2",parse=TRUE) + annotate(geom="text",x=8.1,y=0.08,label="alpha/2",parse=TRUE)
```
 
--

IC : $I(1-\alpha) =  \big[\hat{\theta} - H^{-1}(1-\alpha/2); \hat{\theta} - H^{-1}(\alpha/2)\big]$
 
---
# Construction d'intervalles de confiance
 
Version bootstrap empirique : $\hat{I}^*(1-\alpha) =  \big[\hat{\theta} - (\hat{H}^*_B)^{-1}(1-\alpha/2); \hat{\theta} - (\hat{H}^*_B)^{-1}(\alpha/2)\big]$
 
#### Calcul des quantiles de $\hat{H}^*_B$
 
 - $H^*(x) = G^*(x + \hat{\theta})$
 
--
 
```{r echo=FALSE, fig.width=7,fig.height=5, warning=FALSE,fig.retina=2} 
d <- data.frame(x=rep(x,2),y=c(pgamma(x,2.5,1),pgamma(x-2.5,2.5,1)),type=rep(c("H","G"),each=length(x)),
                xmin=c(qgamma(0.025,2.5,1)),xmax=c(qgamma(0.975,2.5,1)))
ggplot(data=d,aes(x,y,col=type)) + geom_line(lwd=1.5) + scale_color_discrete("Fonction") + xlim(c(0,12)) +
   annotate(geom="text",x=1.4,y=-0.028,label="x",parse=TRUE) + geom_segment(x=1.45,y=0,xend=1.45,yend=0.31,linetype=2,col="black") + geom_segment(x=4,y=0,xend=4,yend=0.31,linetype=2,col="black") +
  annotate(geom="text",x=4,y=-0.01,label="x+hat(theta)",parse=TRUE) + geom_segment(x = 1.45, y = 0.31, xend = 4, yend = 0.31, arrow = arrow(length = unit(0.015, "npc"), ends = "both"), lwd=0.1, col="black") +   annotate(geom="text",x=2.75,y=0.32,label="hat(theta)",parse=TRUE)
```

--
 
 - $(H^*)^{-1}(y) = (G^*)^{-1}(y) - \hat{\theta}$
 
---
# Construction d'intervalles de confiance

#### Méthode du bootstrap classique

$$\hat{I}_{boot}^*(1-\alpha) =  \big[2\hat{\theta} - \hat{\theta}^*_{(\lceil B(1-\alpha/2)\rceil )} ; 2\hat{\theta} - \hat{\theta}^*_{(\lceil B \alpha/2\rceil )}  \big]$$
---
# Construction d'intervalles de confiance

#### Méthode percentile

<span style="color:red">Hypothèse : il existe une transformation $h$ **croissante** telle que la loi de $h(\hat{\theta})$ soit symétrique autour de $\eta = h(\theta)$</span>


IC pour $\eta$ : 
$$IC_\eta(1-\alpha) = \left[U - H^{-1}_U\left( 1 - \frac{\alpha}{2}\right) ; U - H^{-1}_U \left(\frac{\alpha}{2}\right)\right]. $$

 - $h$ inconnue ?
 - $H_U$ inconnue ?

---
# Construction d'intervalles de confiance

Démarche :
 1. construire les échantillons bootstrap $\mathcal{X}_1^*, \dots, \mathcal{X}_B^*$
 2. construire les statistiques bootstrapées $\hat{\theta}^*_1, \dots, \hat{\theta}^*_B$ et leurs transformations par $h$ : $U_b^* = h(\hat{\theta}^*_b), \forall b$
 3. définir la fonction de répartition de la statistique bootstrap : $$H^*_{U} (x) = \mathbb{P}(U_b^* - U \leq x \mid F_n)$$
 4. intervalle de confiance bootstrap pour $\eta = h(\theta)$ : $$IC_\eta^*(1-\alpha) = \left[U - H^{*-1}_{U}\left(1 -\frac{\alpha}{2} \right) ; U - H^{* \ -1}_{U}\left(\frac{\alpha}{2} \right) \right]$$
 
 
 
---
# Construction d'intervalles de confiance

.pull-left[
```{r echo=FALSE, fig.width=5,fig.height=5,warning=FALSE,fig.align='center',fig.retina=2} 
library(ggplot2)
x <- seq(-4,4,0.01)
y <- dnorm(x)
d <- data.frame(x=x,y=y,xmin=qnorm(0.025),xmax=qnorm(0.975))
ggplot(data=d,aes(x,y)) + geom_line() + stat_function(fun = dnorm,geom = "area", fill = "red", alpha = 0.5, xlim = c(-3.5,qnorm(0.025))) + stat_function(fun = dnorm,geom = "area", fill = "red", alpha = 0.5, xlim = c(qnorm(0.975),3.5)) +
  ggtitle(expression(paste("Loi de ",h(hat(theta))-eta))) + xlim(c(-3.5,3.5)) + annotate(geom="text",x=qnorm(0.025)+0.15,y=-0.015,label="H[U]^-1~(alpha/2)",parse=TRUE) +   annotate(geom="text",x=qnorm(0.975),y=-0.015,label="H[U]^-1~(1-alpha/2)",parse=TRUE) + geom_vline(xintercept = 0) + geom_segment(x=qnorm(0.025),xend=qnorm(0.025),y=-0.005,yend=0.005) + geom_hline(yintercept = 0) +  
  geom_segment(x=qnorm(0.975),xend=qnorm(0.975),y=-0.005,yend=0.005) + geom_segment(x=2.5,xend=3.1,y=0.005,yend=0.07,lwd=0.1) + geom_segment(x=-2.5,xend=-3.1,y=0.005,yend=0.07,lwd=0.1) +
  annotate(geom="text",x=-3.2,y=0.08,label="alpha/2",parse=TRUE) + annotate(geom="text",x=3.2,y=0.08,label="alpha/2",parse=TRUE)
```
]
.pull-right[
</br>
</br>
</br>
</br>
$H^{* \ -1}_{U}\left(\frac{\alpha}{2} \right) = - H^{* \ -1}_{U}\left(1 - \frac{\alpha}{2} \right)$
]

$$IC_{\eta}^*(1-\alpha) = \left[U + H^{*-1}_{U}\left(\frac{\alpha}{2} \right) ; U + H^{* \ -1}_{U}\left(1-\frac{\alpha}{2} \right) \right]$$

--

$$IC_{\eta}^*(1-\alpha) = \left[G^{*-1}_{U}\left(\frac{\alpha}{2} \right) ; G^{* \ -1}_{U}\left(1-\frac{\alpha}{2} \right) \right]$$
<span style="color:#16A085">**fin du cours 4 (21/01/2025)**</span>


---
name: c5
# Construction d'intervalles de confiance

#### Méthode percentile

$$\hat{I}_{perc}^*(1-\alpha) =  \big[\hat{\theta}^*_{(\lceil B\alpha/2\rceil )} ; \hat{\theta}^*_{(\lceil B(1- \alpha/2)\rceil )}  \big]$$
</br>
</br>
</br>
</br>
</br>




---
# Construction d'intervalles de confiance

#### Méthode $t$-percentile

Repose sur l'existence d'une quantité pivotale de la forme (de loi notée $J_n$):

$$S_n = \sqrt{n} \ \frac{\hat{\theta} - \theta}{\hat{\sigma}}$$
--

IC classique : $IC(1-\alpha) = \left[\hat{\theta} - \frac{\hat{\sigma}}{\sqrt{n}} J_n^{-1}\left(1-\frac{\alpha}{2}\right) ; \hat{\theta} - \frac{\hat{\sigma}}{\sqrt{n}} J_n^{-1}\left(\frac{\alpha}{2}\right) \right]$


--

Démarche bootstrap :
1. construire les échantillons bootstrap $\mathcal{X}_1^*, \dots, \mathcal{X}_B^*$
2. construire les statistiques bootstrapées $S^*_b = \sqrt{n} \ \frac{\hat{\theta}^*_b - \hat{\theta}}{\hat{\sigma}^*_b}, \quad b=1,\dots,B$
3. approcher $J_n^{-1}$ par les quantiles empiriques des statistiques bootstrapées
	
	
$$\hat{I}_{t-\text{boot}}^*(1-\alpha) =  \left[\hat{\theta} - \frac{\hat{\sigma}}{\sqrt{n}} S^*_{\big(\lceil B(1-\frac{\alpha}{2})\rceil\big)}; \hat{\theta} - \frac{\hat{\sigma}}{\sqrt{n}} S^*_{\big(\lceil B(\frac{\alpha}{2})\rceil \big)} \right]$$

---
# Intervalles de confiance bootstrap

- IC du bootstrap classique :
$$\hat{I}_{boot}^*(1-\alpha) =  \big[2\hat{\theta} - \hat{\theta}^*_{(\lceil B(1-\alpha/2)\rceil )} ; 2\hat{\theta} - \hat{\theta}^*_{(\lceil B \alpha/2\rceil )}  \big]$$

- IC du bootstrap percentile :
 $$\hat{I}_{perc}^*(1-\alpha) =  \big[\hat{\theta}^*_{(\lceil B\alpha/2\rceil )} ; \hat{\theta}^*_{(\lceil B(1- \alpha/2)\rceil )}  \big]$$
 
- IC du bootstrap $t$-percentile :
 $$\hat{I}_{t-\text{boot}}^*(1-\alpha) =  \left[\hat{\theta} - \frac{\hat{\sigma}}{\sqrt{n}} S^*_{(\lceil B(1-\frac{\alpha}{2})\rceil)}; \hat{\theta} - \frac{\hat{\sigma}}{\sqrt{n}} S^*_{(\lceil B(\frac{\alpha}{2})\rceil)} \right]$$


---
# Paramétrique ou non paramétrique ?

- Jusqu'à présent, on a décrit des procédures de bootstrap dit **non-paramétrique**

--

- En effet, on n'a fait aucune hypothèse sur la loi $F$, ni exploité sa forme paramétrique

--

- Au contraire, on a échantillonné les $X_{b,i}^*$ selon la loi $F_n$, qui est un estimateur **non-paramétrique** de $F$

 </br>
 
--

- Il est possible de faire du bootstrap **paramétrique**

--

- Dans ce cas, on va utiliser la forme paramétrique de la loi $F$, que l'on note $F_\theta$.

--

- On échantillonne alors les $X_{b,i}^*$ selon la loi $F_{\hat{\theta}}$, qui est un estimateur **paramétrique** de $F$

<!-- <table> -->
<!--   <tr style="font-weight:bold"> -->
<!--     <td> </td> -->
<!--     <td>Bootstrap non paramétrique</td> -->
<!--     <td>Bootstrap paramétrique</td> -->
<!--   </tr> -->
<!--   <tr> -->
<!--     <td>hypothèse sur $F$</td> -->
<!--     <td>pas d'hypothèse particulière</td> -->
<!--     <td>$F = F_\theta$</td> -->
<!--   </tr> -->
<!-- <tr> -->
<!--     <td>échantillonnage</td> -->
<!--     <td>selon loi $F_n$</td> -->
<!--     <td>selon loi $F_{\hat{\theta}}$</td> -->
<!--   </tr>   -->
<!-- </table> -->

</br>

--

.center[.red[**RQ : c'est parfois la seule approche possible**]]

---
# Tests bootstrap

Procédure de test usuelle pour un test de niveau $\alpha$ :

--

- choix des hypothèses $H_0$ et $H_1$

--

- choix d'une statistique de test $T(\mathcal{X})$

--

- identification de la loi (éventuellement asymptotique) de $T(\mathcal{X})$

--

- déterminer la région de rejet **ou** calculer la $p$-valeur du test

--

- conclure

</br>
--

.center[.red[&rarr;** quelles sont les difficultés possibles ?**]]

---
# Tests bootstrap

Procédure de test usuelle pour un test de niveau $\alpha$ :


- choix des hypothèses $H_0$ et $H_1$ .green[**&rarr; ok**]


- choix d'une statistique de test $T(\mathcal{X})$ .green[**&rarr; ok**]


- identification de la loi (éventuellement asymptotique) de $T(\mathcal{X})$ .red[**&rarr; loi inconnue**]

- déterminer la région de rejet **ou** calculer la $p$-valeur du test .red[**&rarr; non calculable**]


- conclure


---
# Tests bootstrap

#### Exemple avec du bootstrap non paramétrique

$X_1,\dots,X_n$ i.i.d. de loi $F_1$ et $Y_1,\dots,Y_m$ i.i.d. de loi $F_2$.

$$H_0 : F_1 =  F_2 \quad \text{vs.} \quad H_1 : F_1 \neq F_2$$
 
--

On suppose que :
 - l'égalité en loi se traduit par une égalité de certains paramètres
--

 - l'on dispose d'une statistique de test $T(\mathcal{X})$ 

--

</br>
**Objectif** : construire une version bootstrap de $T$ <span style="color:red">**sous $H_0$**</span>

---
# Tests bootstrap

#### Exemple avec du boostrap paramétrique

$X_1,\dots,X_n$ i.i.d. de loi $F_{\theta,\eta}$, et

$$H_0 : \theta =  \theta_0 \quad \text{vs.} \quad H_1 : \theta \neq \theta_0$$

&rarr; $\eta$ est un *paramètre de nuisance*.

</br>
--

Exemple du TRV :

$$ T(X_1,\dots,X_n) = \frac{L(X_1,\dots,X_n ; \hat{\theta}, \hat{\eta}_1)}{L(X_1,\dots,X_n ; \theta_0, \hat{\eta}_0)},$$
</br>
--

<span style="color:red">**&rarr; la loi de $T$ sous $H_0$ n'est pas connue en présence de paramètres de nuisnace**</span>

</br>
<span style="color:#16A085">**fin du cours 5 (27/01/2025)**</span>

---
name: c6
class: inverse, middle, center

# II. Méthodes de Monte-Carlo

---
# Un peu d'histoire ...

- première expérience de type Monte Carlo dûe à Buffon au XVIIIème siècle &rarr; l'aiguille de Buffon

.center[
<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/58/Buffon_needle.svg/1920px-Buffon_needle.svg.png" width="200" align="center">]

--

- les méthodes actuelles sont nées pendant la seconde guerre mondiale au laboratoire américain de Los Alamos &rarr; nom de code *Monte-Carlo*

.center[
<img src="https://frenchriviera.travel/wp-content/uploads/2018/03/Monte-Carlo-Casino1.jpg" width="300" align="center">
]



---
# Objectif

- Question principale : **l'approximation d'intégrales**

$$\int h(x) dx $$

- Outil théorique à la base des méthodes de Monte-Carlo : **la loi forte des grands nombres**

- Outil pratique nécessaire : **la simulation de variables aléatoires**


---

class: inverse, middle, center

# 1. Génération de variables aléatoires

---
# Génération de variables aléatoires

Lois usuelles &rarr; disponibles sous la plupart des langages ou logiciels 

```{r, echo=FALSE}
library(reticulate)
```

```{r}
# Sous R
rexp(n=10, rate=1/5)
```

```{python, eval=FALSE}
# Sous Python
from scipy.stats import expon
expon.rvs(scale=5, size=10)
```

.red[**attention aux conventions utilisées dans chaque langage !**]

---
#  Méthode de la fonction inverse

Repose sur le résultat suivant :
> Soit $F$ une f.d.r. et $F^{-}$ son inverse (généralisée). Soit $U \sim \mathcal{U}([0,1])$. Alors $X = F^{-}(U)$ suit la loi $F$.

--

```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=7, fig.retina=2}
set.seed(10101)
U <- runif(15)
X <- qnorm(U,sd=0.5)

plot(seq(-2,2,0.001),pnorm(seq(-2,2,0.001),sd=0.5),type="l",xlab="",ylab="",yaxt="n")
```


---
#  Méthode de la fonction inverse

Repose sur le résultat suivant :
> Soit $F$ une f.d.r. et $F^{-}$ son inverse (généralisée). Soit $U \sim \mathcal{U}([0,1])$. Alors $X = F^{-}(U)$ suit la loi $F$.


```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=7, fig.retina=2}
plot(seq(-2,2,0.001),pnorm(seq(-2,2,0.001),sd=0.5),type="l",xlab="",ylab="",yaxt="n")
mtext(paste0("U[",1:15,"]"),at=U,side=2,line=2,cex=0.75,las=2)
axis(2,at=sort(U),labels=rep("",15),cex.axis=0.75)
```

---
#  Méthode de la fonction inverse

Repose sur le résultat suivant :
> Soit $F$ une f.d.r. et $F^{-}$ son inverse (généralisée). Soit $U \sim \mathcal{U}([0,1])$. Alors $X = F^{-}(U)$ suit la loi $F$.

```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=7, fig.retina=2}
plot(seq(-2,2,0.001),pnorm(seq(-2,2,0.001),sd=0.5),type="l",xlab="",ylab="",yaxt="n")
mtext(paste0("U[",1:15,"]"),at=U,side=2,line=2,cex=0.75,las=2)
axis(2,at=sort(U),labels=rep("",15),cex.axis=0.75)
mtext(paste0("X[",1:15,"]"),at=X,side=1,line=2,cex=0.75,las=2)
axis(1,at=sort(X),labels=rep("",15),cex.axis=0.75)
segments(-2,U,X,U,lty=2,col="grey")
segments(X,U,X,0,lty=2,col="grey")
```


---
#  Méthode de la fonction inverse

Repose sur le résultat suivant :
> Soit $F$ une f.d.r. et $F^{-}$ son inverse (généralisée). Soit $U \sim \mathcal{U}([0,1])$. Alors $X = F^{-}(U)$ suit la loi $F$.

```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=7, fig.retina=2}
plot(ecdf(X),xlim=c(-2,2),main="",ylab="",yaxt="n")
points(seq(-2,2,0.001),pnorm(seq(-2,2,0.001),sd=0.5),type="l",xlab="",ylab="",yaxt="n")
mtext(paste0("U[",1:15,"]"),at=U,side=2,line=2,cex=0.75,las=2)
axis(2,at=sort(U),labels=rep("",15),cex.axis=0.75)
mtext(paste0("X[",1:15,"]"),at=X,side=1,line=2,cex=0.75,las=2)
axis(1,at=sort(X),labels=rep("",15),cex.axis=0.75)
segments(-2,U,X,U,lty=2,col="grey")
segments(X,U,X,0,lty=2,col="grey")
```


---
# Exemple

Loi de Cauchy, de densité $f(x) = \frac{1}{\pi(1+x^2)}$

--

```{r}
u <- runif(10000)
finv <- function(u){tan(pi*(u-0.5))}
x <- finv(u)
```

--

```{r, echo=FALSE, fig.width=6, fig.height=5, fig.retina=2, fig.align='center', warning=FALSE}
d <- data.frame(x=x)
ggplot(d, aes(x)) +
  geom_histogram(aes(y = after_stat(density)), binwidth = 0.35, fill = "#999999") +
  stat_function(
    fun = dcauchy,
    lwd = 1
  ) + xlim(c(-10,10))
```


---
# Méthode d'acceptation-rejet

Proposition :
> Soit $X$ une v.a. de densité $f$ et soit $g$ une densité de probabilité et une constante $M \geq 1$ t.q. $\forall x, f(x) \leq M g(x)$. Alors pour simuler selon la loi $f$ il suffit de :
1. simuler $Y \sim g$
2. simuler $U | Y=y \sim \mathcal{U}([0,Mg(y)])$
3. si $0 < U < f(Y)$, poser $X=Y$, sinon reprendre l'étape 1.

*Preuve*  <div class="horizontalgap" style="width:10px"></div> 

--

Remarques :
- on a seulement besoin de connaître $f$ à une constante multiplicative près
- $\forall x, f(x) \leq M g(x) \Rightarrow \text{supp}(f) \subset \text{supp}(g)$
- probabilité d'accepter un candidat : $\frac{1}{M}$ (influence du choix de $g$)


<span style="color:#16A085">**fin du cours 6 (28/01/2025)**</span>


---
name: c7
# Méthode d'acceptation-rejet

Illustration

```{r, fig.retina=2, fig.height=7, fig.width=10, echo=FALSE, fig.align='center', warning=FALSE, message=FALSE}
fx <- function(x){0.65*dnorm(x,0,1.1)+0.35*dnorm(x,4,1)}
gx <- function(x){1.83*dnorm(x,1.4,2.5)}
x <- seq(-8,10,length.out=1000)
dx <- data.frame(x=c(x,x),y=c(fx(x),gx(x)),Fonction=c(rep("f",length(x)),rep("Mg",length(x))))
ggplot(data=dx,aes(x=x,y=y,col=Fonction)) + geom_line(size=1) #+ geom_point(x=xg,y=0,cex=4) + 
  #annotate(geom="text",x=xg,y=0.01,label="Y",cex=5) 
```

---
# Méthode d'acceptation-rejet

- Tirage de $Y$ selon la loi $g$

```{r, fig.retina=2, fig.height=7, fig.width=10, echo=FALSE, fig.align='center'}
set.seed(10)
xg <- rnorm(1,1.4,2.5)
ggplot(data=dx,aes(x=x,y=y,col=Fonction)) + geom_line(size=1) + geom_point(x=xg,y=0,cex=4) + 
  annotate(geom="text",x=xg,y=0.01,label="Y1",cex=5) 
```


---
# Méthode d'acceptation-rejet

- Tirage de $U$ conditionnellement à $Y=y$ selon la loi $\mathcal{U}([0,Mg(y)])$ &rarr; on rejette

```{r, fig.retina=2, fig.height=7, fig.width=10, echo=FALSE, fig.align='center'}
set.seed(110)
yg <- runif(1,0,gx(xg))
ggplot(data=dx,aes(x=x,y=y,col=Fonction)) + geom_line(size=1) + geom_point(x=xg,y=yg,cex=4) + geom_segment(x=xg,y=0,xend=xg,yend=gx(xg)) + geom_point(x=xg,y=0,cex=4) + 
  annotate(geom="text",x=xg,y=0.01,label="Y1",cex=5) 
```

---
# Méthode d'acceptation-rejet

- Tirage de $U$ conditionnellement à $Y=y$ selon la loi $\mathcal{U}([0,Mg(y)])$ &rarr; on accepte 

```{r, fig.retina=2, fig.height=7, fig.width=10, echo=FALSE, fig.align='center'}
set.seed(1000)
xg <- rnorm(1,1.4,2.5)
yg <- runif(1,0,gx(xg))
ggplot(data=dx,aes(x=x,y=y,col=Fonction)) + geom_line(size=1) + geom_point(x=xg,y=yg,cex=4) +  geom_segment(x=xg,y=0,xend=xg,yend=gx(xg)) + geom_point(x=xg,y=0,cex=4) + 
  annotate(geom="text",x=xg,y=0.01,label="Y2",cex=5) 
```



---
# Méthode d'acceptation-rejet

A la fin :

```{r, fig.align='center', fig.retina=2, fig.height=7, fig.width=10, echo=FALSE, warning=FALSE, message=FALSE}
library(gganimate)
x <- seq(-8,10,0.01)
fxs <- fx(x)
gxs <- gx(x)
set.seed(0)
xg <- rnorm(5000,1.4,2.5)
yg <- runif(5000,0,gx(xg))
u <- ifelse(yg<fx(xg),1,0)
ddens <- data.frame(x=x,y=fxs,accept=TRUE)
ddens$accept <- factor(ddens$accept,labels=c("Accepté"))
ddens1 <- data.frame(x=x,y=gxs,accept=FALSE)
ddens1$accept <- factor(ddens1$accept,labels=c("Rejeté"))
dpoints <- data.frame(x=xg,y=yg,accept=u)
dpoints$accept <- factor(dpoints$accept,labels=c("Rejeté","Accepté"))
dpoints$n <- rep(seq(100,5000,by=100),each=100)
ggplot(data = dpoints, aes(x=x,y=y,col=accept))  + geom_point(size=0.9) + geom_line(data=ddens, linewidth=1.1) +  geom_line(data=ddens1, linewidth=1.1) + scale_color_hue(direction=-1) #+ scale_color_brewer(name="",palette="Dark2")# + transition_states(n) + shadow_mark() 
```



---
# Exemple 

On veut simuler $X$ selon la loi normale centrée réduite à l'aide d'une loi exponentielle de paramètre 1.

1. par symétrie de la loi normale, il suffit de savoir simuler selon la loi de $|X|$ 

--

2. on génère ensuite $Z$, une Bernoulli de paramètre 1/2 et on pose $X = |X|$ si $Z=1$ et $X=-|X|$ si $Z=0$.

--

3. il faut trouver une constante $M$ telle que $f(x)\leq M g(x)$ pour tout $x$ avec :
- densité cible $f(x) = \frac{2}{\sqrt{2\pi}} e^{-x^2/2} \mathbb{1}_{x\geq 0}$
- densité de la loi exponentielle $g(x) = e^{-x} \mathbb{1}_{x>0}$


---
class: my-one-page-font
# Exemple

```{r}
n <- 1000
f <- function(x){ifelse(x>0,sqrt(2/pi)*exp(-x^2/2),0)}
g <- function(x){ifelse(x>0,exp(-x),0)}

M <- sqrt(2*exp(1)/pi) # env. 1.31 -> 1/M = 0.76
U1 <- runif(n)
Y <- -log(U1)
U2 <- runif(n,min = 0, max = M*g(Y))
absX <- Y[U2<f(Y)]
Z <- 2*rbinom(length(absX),size = 1, p=1/2) - 1
X <- Z*absX
```

--

```{r, echo=FALSE, fig.height=4,fig.width=9,fig.retina=2,fig.align='center'}
#hist(X,breaks = 20,freq = F, main="")
#points(seq(-2.5,2.5,0.001),dnorm(seq(-2.5,2.5,0.001)),col="red",type="l")
x <- seq(0,3,0.001)
dar <- data.frame(x=rep(x,2),y=c(2*dnorm(x),sqrt(2*exp(1)/pi)*dexp(x)),Fonction=rep(c("f","Mg"),each=length(x)))
p1 <- ggplot(data=dar,aes(x,y,color=Fonction)) + geom_line()
p2 <- ggplot(data=data.frame(X=X),aes(x=X)) + geom_histogram(aes(y = after_stat(density)), binwidth = 0.35, fill = "grey",col="darkgrey") + stat_function(fun = dnorm) + ggtitle(paste0("valeurs acceptées : ",length(absX),"/",1000))
gridExtra::grid.arrange(p1,p2,ncol=2)
```

---
# Un cas particulier

Un cas particulier intéressant : le cas $f$ bornée à support compact.

--

- on prend pour $g$ la loi uniforme sur le support de $f$
- et on simule $U$ uniforme sur $[0,m]$ où $m=\max_x f(x)$

--

ex. : $\displaystyle f(x) = \frac{1}{8} |x^4 - 5x^2 + 4| \ \mathbb{1}_{[-2,2]}(x)$
```{r, echo=FALSE, fig.height=6,fig.width=8,fig.align='center',fig.retina=2}
fdens <- function(x){
  ifelse(abs(x)<2,abs(x^4-5*x^2+4)/8,0)
}

curve(fdens,from=-2.5,2.5,lwd=2)
```

---
# Un cas particulier

Un cas particulier intéressant : le cas $f$ bornée à support compact.

- on prend pour $g$ la loi uniforme sur le support de $f$
- et on simule $U$ uniforme sur $[0,m]$ où $m=\max_x f(x)$

ex. : $\displaystyle f(x) = \frac{1}{8} |x^4 - 5x^2 + 4| \ \mathbb{1}_{[-2,2]}(x)$
```{r, echo=FALSE, fig.height=6,fig.width=8,fig.align='center',fig.retina=2}
curve(fdens,from=-2.5,2.5,lwd=2)
curve(2*dunif(x,-2,2),from=-2.5,to=2.5,col="red",lwd=2,add=T)
```

---
# Un cas particulier

$f(x) = \frac{1}{8} |x^4 - 5x^2 + 4| \ \mathbb{1}_{[-2,2]}(x)$

.pull-left[
```{r paged.print=FALSE, eval=FALSE}
Y <- runif(10000,-2,2)
U <- runif(10000,0,0.5)

accept <- (U<fdens(Y))
X <- Y[accept]

hist(X,breaks=50,freq=F)
points(x,fx,type="l",col="red")
```

```{r, echo=FALSE}
Y <- runif(10000,-2,2)
U <- runif(10000,0,0.5)
accept <- (U<fdens(Y))
```

```{r}
mean(accept)
```

&rarr; la moitié des points simulés est 'perdue'
]

.pull-right[
```{r, echo=FALSE}
Y <- runif(10000,-2,2)
U <- runif(10000,0,0.5)

accept <- (U<fdens(Y))
X <- Y[accept]

hist(X,breaks=50,,freq=F,xlab="x",ylab="",main="")
points(seq(-2,2,0.01),fdens(seq(-2,2,0.01)),type="l",col="red",lwd=2)
```
]


---
# Un cas particulier

Visuellement : on peut représenter les points acceptés dans le rectangle $[-2,2] \times [0,1/2]$

</br>

```{r, fig.align='center',fig.height=6,fig.width=8, fig.retina=1,echo=FALSE, warning=FALSE, message=FALSE}
library(gganimate)
x <- seq(-2,2,0.01)
fx <- fdens(x)
ddens <- data.frame(x=x,y=fx,accept=TRUE)
ddens$accept <- factor(ddens$accept,labels=c("Accepté"))
dpoints <- data.frame(x=Y,y=U,accept=accept)
dpoints$accept <- factor(dpoints$accept,labels=c("Rejeté","Accepté"))
dpoints$n <- rep(seq(100,10000,by=100),each=100)
ggplot(data = dpoints, aes(x=x,y=y,col=accept))  + geom_point(size=0.9) + geom_line(data=ddens, linewidth=1.1) + scale_color_discrete(name="") + scale_color_brewer(name="",palette="Dark2")# + transition_states(n) + shadow_mark() 
```

---

class: inverse, middle, center

# 1. Méthodes de Monte-Carlo

---
# Monte Carlo classique

**Objectif** : calculer une intégrale de la forme $\int h(x) f(x) dx$ où $f$ est une densité de probabilité.

--

> *Définition.* Soit $X_1,\dots,X_n$ un échantillon i.i.d. de loi $f$. L'estimateur de Monte Carlo de $\mathbb{E}(h(X))$ est :
$$\hat{h}_n = \frac{1}{n} \sum_{i=1}^n h(X_i)$$


Propriétés :
- estimateur sans biais
- fortement consistant
- IC avec le TCL

Remarques :
- vitesse de convergence en $\sqrt{n}$, indépendante de la dimension
- ne dépend pas de la régularité de $h$

---
# Exemples d'applications

#### Approcher la $p$-valeur d'un test

ex. avec le test du rapport de vraisemblance d'un modèle gaussien :
$$H_0 : \quad (\mu,\sigma^2)=(\mu_0,\sigma_0^2)\quad\textrm{ contre }\quad H_1 : \quad (\mu,\sigma^2)\neq(\mu_0,\sigma_0^2).$$

--

On a la statistique de test suivante (voir DS de Stat Math 2024) : 
$$2\ln V_n= T_n^2-n\ln T_n^2 + Z^2 + n\ln n -n,$$

avec $Z=\sqrt{n}\big(\frac{\hat{\mu}_n-\mu_0}{\sigma_0}\big) \quad \textrm{et}\quad T_n^2=\frac{n\hat{\sigma}^2_n}{\sigma_0^2}$.

Problème : la loi de la statistique de test n'est pas connue (tabulée).

---
# Exemples d'applications

Les données :
```{r, echo=FALSE}
x <- c(98.23,97.91,98.24,99.00,102.64,103.44,103.81,100.64,100.86,98.79,103.48, 98.10,101.11, 98.30,96.56,98.77,100.15,101.58,97.51,100.87,101.99, 98.59,98.72,103.97,94.75,97.92,102.30, 96.88,101.44,100.12)
x
n <- length(x)
```
--

.pull-left[
```{r}
Tn2 <- (n-1)*var(x)/4
V_obs <- Tn2 - n*log(Tn2) + n*(mean(x)-100)^2/4 + n*log(n) - n
print(V_obs)

N <- 100000
T <- rchisq(N,n-1)   
Z <- rnorm(N,0,1)    
V <- T - n*log(T) + Z^2 + n*log(n) - n
p_val <- mean(V > V_obs)
print(p_val)
```
]
.pull-right[
```{r, echo=FALSE, warnings=FALSE,message=FALSE}
ggplot(data=NULL,aes(x=V)) + geom_histogram(aes(y=after_stat(density)),fill="grey",col="darkgrey") + geom_vline(xintercept = V_obs, col="red")
```
]

---
# Exemples d'applications

On peut quantifier l'erreur d'approximation :

.pull-left[
```{r paged.print=FALSE, eval=FALSE}
N <- c(100,1000,2000,5000,10000,20000,
       50000,100000,200000,300000)
p_val <- rep(0,length(N))
for (i in 1:length(N)){
  T <- rchisq(N[i],n-1)   
  Z <- rnorm(N[i],0,1)    
  V <- T-n*log(T)+Z^2+n*log(n)-n
  p_val[i] <- mean(V > V_obs)
}
```
]
.pull-right[
```{r, echo=FALSE, fig.width=6,fig.height=4, fig.retina=2}
N <- c(100,1000,2000,5000,10000,20000,
       50000,100000,200000,300000)

p_val <- rep(0,length(N))
for (i in 1:length(N)){
  T <- rchisq(N[i],n-1)   
  Z <- rnorm(N[i],0,1)    
  V <- T-n*log(T)+Z^2+n*log(n)-n
  p_val[i] <- mean(V > V_obs)
}
d <- data.frame(phat = p_val, lb = p_val-qnorm(0.975)*sqrt(p_val*(1-p_val)/N), ub = p_val+qnorm(0.975)*sqrt(p_val*(1-p_val)/N), n = N)
ggplot(data=d,aes(n,phat)) + geom_line() + geom_line(aes(n,lb),col="red") + geom_line(aes(n,ub),col="red") + ylab("Estimation de p") 
```
]

---
# Exemples d'application

Plus généralement, les méthodes de Monte Carlo s'utilisent pour :

- approcher le niveau ou la puissance d'un test

- approcher une intégrale en grande dimension

- faire de l'optimisation (e.g. algorithme du recuit simulé, descente de gradient stochastique, ...)

- ...

<span style="color:#16A085">**fin du cours 7 (03/02/2025)**</span>

