---
title: "Statistiques computationnelles"
author: <font size="5"> Charlotte Baey </font>
date: <font size="5"> M1 MA - 2025/2026 </font>
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, hygge]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%"
---

<style>

.remark-slide-content {
  background-color: #FFFFFF;
  border-top: 80px solid #16A085;
  font-size: 20px;
  line-height: 1.5;
  padding: 1em 2em 1em 2em
}

.my-one-page-font {
  font-size: 20px;
}

.remark-slide-content > h1 {
  font-size: 38px;
  margin-top: -85px;
}

.inverse {
  background-color: #16A085;
  border-top: 80px solid #16A085;
  text-shadow: none;
	background-position: 50% 75%;
  background-size: 150px;
  font-size: 40px
}

.title-slide {
  background-color: #16A085;
  border-top: 80px solid #16A085;
  background-image: none;
}

.remark-slide-number {
  position: absolute;
}

.remark-slide-number .progress-bar-container {
  position: absolute;
  bottom: 0;
  height: 4px;
  display: block;
  left: 0;
  right: 0;
}

.remark-slide-number .progress-bar {
  height: 100%;
  background-color: grey;
}

.left-column {
  width: 20%;
  height: 92%;
  float: left;
}
.left-column h2:last-of-type, .left-column h3:last-child {
  color: #000;
}
.right-column {
  width: 75%;
  float: right;
  padding-top: 1em;
}


.left-column2 {
  width: 60%;
  height: 92%;
  float: left;
}
.right-column2 {
  width: 35%;
  height: 92%;
  float: left;
}

</style>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      yellow: ["{\\color{yellow}{#1}}", 1],
      orange: ["{\\color{orange}{#1}}", 1],
      green: ["{\\color{green}{#1}}", 1]
    },
    loader: {load: ['[tex]/color']},
    tex: {packages: {'[+]': ['color']}}
  }
});
</script>

```{css, echo=FALSE}
.left-code {
  width: 40%;
  height: 92%;
  float: left;
}
.right-plot {
  width: 59%;
  float: right;
  padding-left: 1%;
}
```

```{css, echo=FALSE}
.left-plot {
  width: 59%;
  float: left;
}
.right-code {
  width: 40%;
  float: right;
  padding-left: 1%;
}
```


```{r setup, include = FALSE}

# R markdown options
#knitr::opts_chunk$set(echo = TRUE, 
                      #cache = TRUE, 
                      #fig.width = 10,
                      #fig.height = 5,
                      #fig.align = "center", 
                      #message = FALSE)

# Load packages
library(gapminder)
library(gganimate)
library(ggplot2)
library(tidyverse)
library(xaringancolor)
library(flair)
yellow <- "#FFCC29"
orange <- "#F58634"
green <- "#007965"
cprior <- "#ffe933"
cobs <- "#2a6099"
cpost <- "#77bc65"
```

```{r, echo=FALSE}
#library(reticulate)
#Sys.setenv('RETICULATE_PYTHON'='/usr/bin/python3')
```

# Quelques informations pratiques

### Plan du cours
1. Méthodes de ré-échantillonnage
2. Méthodes de Monte-Carlo
3. Introduction aux statistiques bayésiennes
4. Algorithme EM (s'il reste du temps)

### Organisation

- 2 séances de cours d'1h30 par semaine ($\times$ 11 semaines)
- 2 séances de TD/TP de 2h par semaine ($\times$ 12 semaines)

### Evaluation

- 1 DS intermédiaire d'une durée de 2h
- 1 Projet **à effectuer en binôme**
- 1 DS final d'une durée de 3h

.red[**Aucun document autorisé lors des examens.**]

---
# Sommaire

<!-- .pull-left[ -->
**1. Méthodes de ré-échantillonnage**
  - [Cours 1](#c1) (13/01/2025)
  
<!-- - [Cours 2](#c2) (14/01/2025)
  - [Cours 3](#c3) (20/01/2025)
  - [Cours 4](#c4) (21/01/2025)
  - [Cours 5](#c5) (27/01/2025)
  
**2. Méthodes de Monte-Carlo**
  - [Cours 6](#c6) (28/01/2025)
  - [Cours 7](#c7) (03/02/2025)
  - [Cours 8](#c8) (04/02/2025)
  - [Cours 9](#c9) (10/02/2025)
  - [Cours 10](#c10) (11/02/2025)

**3. Méthodes MCMC**
  - [Cours 11](#c11) (24/02/2025)
  - [Cours 12](#c12) (25/02/2025)
]
.pull-right[
**3. Méthodes MCMC (suite)**
  - [Cours 13](#c13) (04/03/2025)
  - [Cours 14](#c14) (10/03/2025)
  
** 4. Statistiques bayésiennes**
  - [Cours 15](#c15) (11/03/2025)
  - Cours 16 (pas de slides) (17/03/2025)
  - [Cours 17](#c16) (18/03/2025)
  - [Cours 18](#c18) (24/03/2025)
  - [Cours 19](#c19) (25/03/2025)
  - [Cours 20](#c20) (31/03/2025)
]
-->
---
name: c1
class: inverse, middle, center

# Introduction


---
class: my-one-page-font 
# C'est quoi les statistiques computationnelles ?

<br>

<br>

 - C'est le recours (plus ou moins) intensif à l'ordinateur pour répondre à des questions statistiques que l'on ne sait pas (ou difficilement) résoudre autrement.
 
 - On utilise/développe/étudie des algorithmes, des astuces numériques/statistiques/computationnelles 
 - L'objectif est de faire de l'inférence, d'étudier la robustesse de méthodes statistiques, de traiter de grands jeux de données, ...
 
 

---

class: inverse, middle, center

# I. Méthodes de ré-échantillonnage

---

# Notion d'échantillon

Qu'est-ce qu'un échantillon ?

- une suite de variables aléatoires $\mathcal{X} = (X_1, \dots, X_n)$
- dont on observe une réalisation $\mathcal{X}(\omega) = (X_1(\omega), \dots, X_n(\omega))$

---
# Notion d'échantillon

Qu'est-ce qu'un échantillon ?

- une suite de variables aléatoires $\mathcal{X} = (X_1, \dots, X_n)$
- dont on observe une réalisation $\mathcal{X}(\omega) = (X_1(\omega), \dots, X_n(\omega))$

Que représente $\omega$ ?
- l'aléa autour de l'expérience (ex. : $n$ lancers d'une pièce de monnaie)
- cet aléa $\omega \in \Omega$ est transporté dans $\mathbb{R}$ via $X_i$
<!-- - on a seulement accès à la variabilité sur $\mathbb{R}$ -->
- en général, on ne dispose que d'une seule réalisation, pour un $\omega$ donné

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=34)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=35)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=36)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=37)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=38)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=39)
```


---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=40)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=41)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=42)
```

---
# Statistique paramétrique

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=43)
```


---
# Un exemple simple 

Soit $(X_1,\dots,X_n)$ un échantillon gaussien i.i.d. de loi $\mathcal{N}(\theta,1)$, et $\hat{\theta}$ l'EMV de $\theta$. Quelle est la loi de $\hat{\theta}$ ?

--

.pull-left[
```{r, eval=F}
theta_vrai <- 2; n <- 100; N <- 500
hat_theta <- rep(0,N)
for (i in 1:N){
  ech_i <- rnorm(n,theta_vrai,1)
  hat_theta[i] <- mean(ech_i)
}
hist(hat_theta,freq=F)
curve(dnorm(x,theta_vrai,1/sqrt(n)))
```
]
.pull-right[
```{r, fig.height=5,fig.width=6,fig.retina=2,results='hold',echo=F}
theta_vrai <- 2; n <- 100; N <- 500
hat_theta <- rep(0,N)
for (i in 1:N){
  ech_i <- rnorm(n,theta_vrai,1)
  hat_theta[i] <- mean(ech_i)
}
hist(hat_theta,freq=F,main="Histogramme des estimations",ylim=c(0,4))
curve(dnorm(x,theta_vrai,1/sqrt(n)),
      lwd=2,col="red",add=T)
```
]
---
class: my-one-page-font 
# Jackknife

Comment construire de nouveaux échantillons ?

```{r, fig.height=6,fig.width=10,fig.retina=2, echo=F}
library(RColorBrewer)
pal <- brewer.pal(n = 8, name = "Dark2")
n <- 8
set.seed(2)
ech <- sort(rnorm(n,0,1))
ech_j <- matrix(0,nrow=n,ncol=7)
for (i in 1:n){
  ech_j[i,] <- ech[-i]
}

mean_ech <- mean(ech)
mean_j <- apply(ech_j,1,mean)
par(oma=c(0,4,0,0))
plot(ech,rep(1.9,n),col=pal,pch=19,cex=4,ylim=c(0.1,2),yaxt="none",xlab="Observations",ylab="")
points(mean_ech,1.9,col="red",pch=15,cex=2)
mtext("éch. initial",side=2,at=1.9,las=1,adj=1.1)
```

---
class: my-one-page-font 
# Jackknife

Comment construire de nouveaux échantillons ?

```{r, fig.height=6,fig.width=10,fig.retina=2, echo=F}
par(oma=c(0,4,0,0))
plot(ech,rep(1.9,n),col=pal,pch=19,cex=4,ylim=c(0.1,2),yaxt="none",xlab="Observations",ylab="")
points(mean_ech,1.9,col="red",pch=15,cex=2)
mtext("éch. initial",side=2,at=1.9,las=1,adj=1.1)
for (i in 1:n){
  abline(h=1.8-0.2*i,col="grey")
  points(ech_j[i,],rep(1.8-0.2*i,n-1),col=pal[-i],cex=3,pch=19)
  points(mean_j[i],1.8-0.2*i,col="red",pch=15,cex=2)
  mtext(paste("éch. jackknife",i),side=2,at=1.8-0.2*i,las=1,adj=1.1)
}
```


---
# Jackknife 

#### 1. Réduction du biais

 - Estimation du biais : 

$$\hat{b}_{jack} = (n-1)(\hat{\theta}_{jack} - \hat{\theta}),$$ avec $\hat{\theta}_{jack} = \frac{1}{n}\sum_{i=1}^n \hat{\theta}_{(i)}$.

 - Pseudo-valeurs :

$$\tilde{\theta}_{(i)} = n\hat{\theta} - (n-1)\hat{\theta}_{(i)}$$
<!-- Sur l'exemple précédent, cela donne : -->
 <!-- \tilde{\theta}_{(1)} = \tilde{\theta}_{(2)} = \tilde{\theta}_{(3)} = \tilde{\theta}_{(7)} = \tilde{\theta}_{(8)} = \tilde{\theta}_{(10)} = \tilde{\theta}_{(12)} = 13\times0.5384 - 12\times0.50 = 0.9992
 \tilde{\theta}_{(4)} = \tilde{\theta}_{(5)} = \tilde{\theta}_{(6)} = \tilde{\theta}_{(9)} = \tilde{\theta}_{(11)} = \tilde{\theta}_{(13)} = 13\times0.5384 - 12\times0.5833 = -0.0004 -->


 - Estimateur jackknife corrigé du biais :

$$\hat{\theta}^*_{jack} = \hat{\theta} - \hat{b}_{jack} = \frac{1}{n}\sum_{i=1}^n \tilde{\theta}_{(i)}$$
.red[**Réduire le biais n'implique pas nécessairement une amélioration de l'estimateur (au sens du risque quadratique)**]


---
# Jackknife

#### 2. Estimation de la variance

 $$\hat{s}^2_{jack} = \frac{n-1}{n} \sum_{i=1}^n \big(\hat{\theta}_{(i)} - \hat{\theta}^*_{jack}\big)^2$$
 
avec les pseudo-valeurs : $\hat{s}^2_{jack} = \frac{1}{n(n-1)} \sum_{i=1}^n \big(\tilde{\theta}_{(i)} - \tilde{\theta}\big)^2$

--

#### 3. Construction d'intervalles de confiance

Si existence d'un TCL :
- en utilisant l'estimateur jackknife de la variance : 
 $$\hat{I}_{jack} = \left[\hat{\theta} - q^{\mathcal{N(0,1)}}_{1-\alpha/2} \hat{s}_{jack} ; \hat{\theta} + q^{\mathcal{N(0,1)}}_{1-\alpha/2} \hat{s}_{jack} \right]$$

- en utilisant les deux estimateurs :
  $$\hat{I}_{jack} = \left[\hat{\theta}_{jack} - q^{\mathcal{N(0,1)}}_{1-\alpha/2} \hat{s}_{jack} ; \hat{\theta}_{jack} + q^{\mathcal{N(0,1)}}_{1-\alpha/2} \hat{s}_{jack} \right]$$



---
name: c2
# Résumé et remarques

<br>
- le jackknife est une méthode **non-paramétrique** permettant d'estimer le biais et la variance d'un estimateur à l'aide de simulations 

- la consistance est garantie pour un grand nombre d'estimateurs **suffisamment réguliers**

- les hypothèses de régularité sont toutefois plus strictes que pour un TCL par exemple :
 - la delta-méthode requiert la différentiabilité de $g$ en $\mu=\mathbb{E}(X_1)$
 - la consistance de $\hat{s}_{jack}$ requiert que $g'$ soit continue en $\mu$
 - ex. d'estimateur pour lequel $\hat{s}_{jack}$ n'est pas consistant : la médiane empirique (malgré existence TCL)
 
- extensions possibles reposant sur des hypothèses moins fortes : 
 - le *delete-d* jackknife
 - le jackknife infinitésimal
 <!-- - peut-être utilisé pour la médiane empirique -->
 <!-- - peut permettre d'estimer la distribution de $\hat{\theta}$ -->


---
# Du jackknife au bootstrap

 - $\hat{\theta} = T(\underbrace{X_1,\dots,X_n}_{\text{observations}},\underbrace{\omega_1,\dots,\omega_n}_{\text{poids des obs.}}) = T(X_1,\dots,X_n,\frac{1}{n},\dots,\frac{1}{n})$
 
 - réplication jackknife : $\hat{\theta}_{(i)} = T(X_1,\dots,X_{i-1},X_i,X_{i+1},X_n,\frac{1}{n-1},\dots,\frac{1}{n-1},0,\frac{1}{n-1},\dots,\frac{1}{n-1})$

--

 - Idée du bootstrap : mettre des poids **aléatoires**

--

 - Procédure de ré-échantillonnage : tirer uniformément et **avec remise** parmi les observations de $\mathcal{X}$, pour construire un échantillon *bootstrap* de taille $n$ noté $\mathcal{X}^*$ :
 
 - Puis sur chaque échantillon bootstrap, on construit la statistique bootstrapée $\hat{\theta}^* = T(\mathcal{X}^*)$


---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=25)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=26)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=27)
```


---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=28)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=29)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=30)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/slides.pdf",pages=31)
```

---
# Bootstrap

```{r, out.width='2000px', echo = FALSE}
magick::image_read_pdf("/home/charlotte/Documents/Enseignement/Lille/M1 ISN /Statistiques computationnelles/Poly/Figures/boot_samples.pdf")
```



---
# Fonction de répartition empirique

 
```{r, echo=FALSE}
set.seed(10101)
xi <- rnorm(10)
print(round(xi,2))
```

--

```{r, echo=FALSE, warning=FALSE, fig.height=7, fig.width=10,fig.retina=2}
plot(ecdf(xi),xaxt='n',main="",col="white")
mtext(round(xi,2),at=xi,side=1,line=1,cex=0.8)
mtext(paste0("X(",1:10,")"),at=xi,side=1,line=2,cex=0.75)
axis(1,at=sort(xi),labels=rep("",10),cex.axis=0.75)
```

---
# Fonction de répartition empirique

 
```{r, echo=FALSE}
set.seed(10101)
xi <- rnorm(10)
print(round(xi,2))
```


```{r, echo=FALSE, warning=FALSE, fig.height=7, fig.width=10,fig.retina=2}
plot(ecdf(xi),xaxt='n',main="")
mtext(round(xi,2),at=xi,side=1,line=1,cex=0.8)
mtext(paste0("X(",1:10,")"),at=xi,side=1,line=2,cex=0.75)
axis(1,at=sort(xi),labels=rep("",10),cex.axis=0.75)
```

---

# Fonction de répartition empirique

 - Echantillon exponentiel
 
```{r, echo=FALSE, warning=FALSE, fig.height=4, fig.width=16,fig.retina=2}
n <- c(10,20,50,100,200,500)
d <- lapply(n,FUN = function(i){return(data.frame(x=rexp(i*100,1),rep=rep(1:100,each=i),n=rep(i,i*100)))})
d <- do.call(rbind,d)

d %>% ggplot(aes(x,group=rep)) + stat_ecdf(geom="step",col="darkgrey") + stat_function(fun=pexp) + facet_wrap(~n,nrow = 1) 
```

 - Echantillon uniforme
 
```{r, echo=FALSE, warning=FALSE, fig.width=16, fig.height=4,fig.retina=2}
d <- lapply(n,FUN = function(i){return(data.frame(x=runif(i*100),rep=rep(1:100,each=i),n=rep(i,i*100)))})
d <- do.call(rbind,d)

d %>% filter(n<1000) %>% ggplot(aes(x,group=rep)) + stat_ecdf(geom="step",col="darkgrey") + stat_function(fun=punif) + facet_wrap(~n,nrow=1) + xlim(c(-0.2,1.2))
```

---
# Bootstrap

.pull-left[
### Monde réel
.left[
<span style="color:white">**on raisonne conditionnellement à $F_n$**</span>
- échantillon $\mathcal{X} = (X_1,\dots,X_n)$
- $X_i$ de loi inconnue $F$
- paramètre $\theta(F)$
- estimateur $\hat{\theta} = T(\mathcal{X})$
- loi de $\hat{\theta}$ : $G$ inconnue
]
]
.pull-right[
### Monde Bootstrap 
.left[
<span style="color:red">**on raisonne conditionnellement à $F_n$**</span>
- échantillon bootstrap $\mathcal{X}^* = (X_{1}^*,\dots,X_{n}^*)$
- $X_{i}^*$ de loi connue $F_n$
- paramètre $\theta(F_n) = \hat{\theta}$
- statistique bootstrapée $\hat{\theta}^* = T(\mathcal{X}^*)$
- loi de $\hat{\theta}^*$ : $G^*$ connue
]
]

</br>
.center[
$G$ inconnue $\longrightarrow$ $G^*$ connue $\longrightarrow$ $\hat{G}^*_B$ approximation bootstrap
]

</br>

